# ðŸ§  LLMs â€” Handsâ€‘on Notebooks for Modern Large Language Models

A lightweight, noâ€‘nonsense collection of Jupyter notebooks exploring core LLM concepts and practical fineâ€‘tuning. If youâ€™re here to *learn by doing* â€” youâ€™re in the right place.


---

## Table of Contents

* [Project Goals](#project-goals)
* [Repo Structure](#repo-structure)
* [Prerequisites](#prerequisites)
* [Quickstart](#quickstart)
* [Notebook Guides](#notebook-guides)
* [Datasets](#datasets)
* [Hardware Notes](#hardware-notes)
* [Common Pitfalls](#common-pitfalls)
* [Roadmap](#roadmap)
* [Contributing](#contributing)
* [License](#license)

---

## Project Goals

* Build intuition for the Transformer architecture from scratch.
* Fineâ€‘tune popular open and openâ€‘ish models with minimal boilerplate.
* Compare training techniques: full fineâ€‘tuning vs LoRA/QLoRA.
* Keep the code *readable first*, *fast second*.

---

## Repo Structure

```
LLMs/
â”œâ”€â”€ BERT.ipynb                    # Encoder-only model: classification with BERT
â”œâ”€â”€ LLM(transformer).ipynb       # From-scratch Transformer walkthrough (educational)
â”œâ”€â”€ Finetuning_LLama2.ipynb      # Instruction/chat fine-tuning for Llama 2
â”œâ”€â”€ Gemma_tuning_LoRA.ipynb      # Parameter-efficient tuning (LoRA) on Gemma
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE                      # MIT
â””â”€â”€ README.md                    # You are here
```

---

## Prerequisites

* **Python** â‰¥ 3.10
* **GPU**: NVIDIA CUDAâ€‘capable GPU recommended (>= 12 GB VRAM for most fineâ€‘tunes; 24 GB+ preferred). CPU-only works for the BERT demo, but bring snacks.
* **Key libraries** (installed in Quickstart):

  * `torch`, `transformers`, `datasets`, `accelerate`
  * `peft` (for LoRA/QLoRA), `bitsandbytes` (optional 4â€‘bit)
  * `scikit-learn`, `evaluate`, `numpy`, `pandas`
  * `trl` (optional, for SFT/RLHF experiments)

> **Model Access:** Some model weights (e.g., *meta-llama/Llama-2-7b-chat-hf*) require access approval on Hugging Face. Request access on the model card with the same HF account that your token uses. After approval, login with `huggingface-cli login` and restart the notebook kernel.

---

## Quickstart

### 1) Create and activate an environment

```bash
# conda (recommended)
conda create -n llms python=3.10 -y
conda activate llms

# or uv
# curl -LsSf https://astral.sh/uv/install.sh | sh
# uv venv --python 3.10
# source .venv/bin/activate
```

### 2) Install dependencies

```bash
pip install --upgrade pip
pip install torch --index-url https://download.pytorch.org/whl/cu121  # adjust CUDA version as needed
pip install transformers datasets accelerate peft bitsandbytes scikit-learn evaluate pandas numpy tqdm sentencepiece safetensors
# optional extras
pip install trl wandb
```

### 3) Authenticate to Hugging Face (if using gated models)

```bash
huggingface-cli login
```

### 4) Run notebooks

Open the notebooks in VS Code (.ipynb) or Jupyter Lab. Execute cells topâ€‘toâ€‘bottom.

---

## Notebook Guides

### 1) `LLM(transformer).ipynb`

**What youâ€™ll learn:**

* Tokenization â†’ embeddings â†’ positional encodings â†’ selfâ€‘attention â†’ MLP â†’ logits.
* Causal masking and why your model shouldnâ€™t leak the future.
* Clean, minimal PyTorch blocks (attention, feedâ€‘forward, residuals).

**Outcome:** A *from-scratch* miniâ€‘Transformer you can extend.

---

### 2) `BERT.ipynb`

**Task:** Text classification on GLUE/SSTâ€‘2 (or similar) using an encoderâ€‘only architecture.

**Covers:**

* Loading datasets via `datasets.load_dataset`.
* Fineâ€‘tuning `bert-base-uncased` with `Trainer` or `Accelerate`.
* Metrics with `evaluate` (accuracy, F1) and a quick confusion matrix.

**Outcome:** A strong baseline and a mental model for encoderâ€‘only LMs.

---

### 3) `Finetuning_LLama2.ipynb`

**Task:** Supervised fineâ€‘tuning (SFT) for chat/instruction tasks on Llamaâ€‘2.

**Covers:**

* Dataset formatting for chat templates (`apply_chat_template`).
* Mixedâ€‘precision (`bf16`/`fp16`), gradient accumulation, and checkpointing.
* Paged optimizers & memoryâ€‘savvy configs.

**Notes:** Requires HF access to `meta-llama/*`. Swap in Mistral or Qwen if access is pending.

**Outcome:** A solid SFT run with reproducible training args and eval samples.

---

### 4) `Gemma_tuning_LoRA.ipynb`

**Task:** Parameterâ€‘efficient fineâ€‘tuning (PEFT) with LoRA on Googleâ€™s Gemma series.

**Covers:**

* `peft.LoraConfig`, target modules, and rank selection.
* 8â€‘bit/4â€‘bit loading via `bitsandbytes` to fit big models on small GPUs.
* Merging LoRA weights back into the base model for export.

**Outcome:** Cheap(â€‘ish) training that still moves the needle.

---

## Datasets

* **SSTâ€‘2 (GLUE)** for classification demos.
* Example instruction datasets (Alpacaâ€‘style, OpenHermesâ€‘style) â€” swap your own by matching the schema used in the notebooks.

**Tip:** Keep raw data in `data/` (ignored by Git) and log training runs to `wandb` or a simple CSV.

---

## Hardware Notes

* **12â€“16 GB VRAM**: BERT fineâ€‘tuning, small LoRA runs (4â€‘bit).
* **24 GB VRAM**: Comfortable LoRA on 7B models.
* **48 GB+ VRAM / A100**: Fullâ€‘precision or larger batch sizes.
* Mixed precision (`bf16` on Ampere+) saves memory and usually speeds things up.

---

## Common Pitfalls

* **Model access denied**: Request on HF *and* login in the same environment; restart the kernel.
* **CUDA OOM**: Use `load_in_8bit/4bit`, reduce `batch_size`, enable gradient accumulation, shorten sequence length.
* **Tokenizer mismatch**: Always `AutoTokenizer.from_pretrained(same_model_id)` and save with `save_pretrained`.
* **Training too slow**: Turn on `torch.set_float32_matmul_precision("high")`, use `gradient_checkpointing`.

---

## Roadmap

* [ ] Add Mistral/Qwen SFT notebook
* [ ] Add QLoRA variant with paged optimizers
* [ ] Add evaluation harness (perplexity, exactâ€‘match, Rougeâ€‘L)
* [ ] Add small inference API (FastAPI + `transformers`/`vLLM`)

---

## Contributing

PRs welcome â€” especially improvements to clarity, comments, and reproducibility. If something breaks, open an issue with your **GPU model**, **driver/CUDA**, **PyTorch version**, and a **minimal repro**. Logs > guesses.

---

## License

This project is released under the **MIT License**. See `LICENSE` for details.
